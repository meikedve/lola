{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbJIUrLaexxM"
      },
      "source": [
        "#Preparations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eamw8ceiZuxf",
        "outputId": "415f6675-bab2-4190-8132-7058f5656670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPJOXeLDbWv0",
        "outputId": "e35f5be0-cb70-4c8e-82db-0b3e07e9b1e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'assigntools'...\n",
            "remote: Enumerating objects: 259, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 259 (delta 25), reused 7 (delta 3), pack-reused 200 (from 1)\u001b[K\n",
            "Receiving objects: 100% (259/259), 63.63 KiB | 5.30 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "Requirement already satisfied: svgling in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.11/dist-packages (from svgling) (1.4.3)\n",
            "--2025-02-01 16:36:24--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip.2’\n",
            "\n",
            "snli_1.0.zip.2      100%[===================>]  90.17M  34.0MB/s    in 2.7s    \n",
            "\n",
            "2025-02-01 16:36:27 (34.0 MB/s) - ‘snli_1.0.zip.2’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "replace snli_1.0/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n",
            "Found .json files for ['dev', 'test', 'train'] parts\n",
            "processing DEV:\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10000it [00:02, 3520.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "158 problems without a gold label were ignored\n",
            "0 problems have a wrong annotator label\n",
            "9842 problems were returned\n",
            "processing TEST:\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10000it [00:02, 4025.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "176 problems without a gold label were ignored\n",
            "0 problems have a wrong annotator label\n",
            "9824 problems were returned\n",
            "processing TRAIN:\t"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "550152it [00:59, 9229.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "785 problems without a gold label were ignored\n",
            "198 problems have a wrong annotator label\n",
            "549169 problems were returned\n",
            "Most common wrong annotator labels: //(198)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#the code and comments in this cell are copied from LoLa tools.ipynb\n",
        "!rm -fr assigntools # helps to rerun this cell witthout errors, if recloning needed\n",
        "! git clone https://github.com/kovvalsky/assigntools.git\n",
        "! pip install svgling\n",
        "from assigntools.LoLa.read_nli import snli_jsonl2dict, sen2anno_from_nli_problems\n",
        "from assigntools.LoLa.sen_analysis import spacy_process_sen2tok, display_doc_dep\n",
        "from nltk.tree import Tree\n",
        "# Get SNLI data on fly\n",
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip\n",
        "SNLI, S2A = snli_jsonl2dict('snli_1.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper functions and definitions"
      ],
      "metadata": {
        "id": "iWK1b81toUtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Our definitions"
      ],
      "metadata": {
        "id": "5CS7XJKJpD-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "adj_adv = [\"JJ\",\"JJR\",\"JJS\",\"RB\",\"RBR\",\"RBS\",\"CD\",\"PDT\"]\n",
        "nouns = ['NN','NNS','NNPS','NNP']\n",
        "extra_hyponyms = {\"people\": [\"men\",\"women\",\"girls\",\"boys\",\"guys\",\"children\",\"kids\",\"adults\"]}\n",
        "\n",
        "male = [\"men\",\"boys\",\"man\",\"boy\",\"male\",\"guys\"]\n",
        "female = [\"girls\",\"girl\",\"women\",\"woman\",\"female\",\"lady\"]\n",
        "adults = [\"adult\",\"adults\",\"man\",\"woman\",\"men\",\"women\",\"lady\"]\n",
        "kids = [\"children\",\"kids\",\"child\",\"toddler\"]\n",
        "dogs = [\"puppies\",\"puppy\",\"dogs\",\"dog\"]\n",
        "cats = [\"cat\",\"cats\"]\n",
        "animals = dogs + cats\n",
        "people = [\"guys\",\"mother\",\"girls\",\"boys\"] + adults + kids\n",
        "\n",
        "extra_antonyms = {\"men\": kids + female + animals,\n",
        "                  \"guys\": female + animals,\n",
        "                  \"women\": kids + male + animals,\n",
        "                  \"mother\": [\"male\",\"men\",\"man\",\"guys\"] + animals,\n",
        "                  \"girls\": male + animals,\n",
        "                  \"boys\": female + animals,\n",
        "                  \"dog\": cats + people,\n",
        "                  \"cat\": dogs + people,\n",
        "                  \"dogs\": cats + people,\n",
        "                  \"cats\": dogs + people,\n",
        "                  \"child\": adults + animals,\n",
        "                  \"adult\": kids + animals,\n",
        "                  \"children\": adults + animals,\n",
        "                  \"adults\": kids + animals,\n",
        "                  \"kids\": adults + animals\n",
        "                  }"
      ],
      "metadata": {
        "id": "q6cXzxukn5D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Preparation functions"
      ],
      "metadata": {
        "id": "gy96Vb1dpUYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(p, h, dictionary):\n",
        "    \"\"\"Returns the premise and hypothesis with only words that contain content.\"\"\"\n",
        "\n",
        "    no_content = {'DT', 'IN', 'CC', 'TO', 'PRP', 'PRP$'}\n",
        "    auxilaries = {\"be\", \"have\", \"do\", \"will\", \"shall\", \"can\", \"may\", \"must\", \"go\"}\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def get_uninflected_form(word):\n",
        "        return lemmatizer.lemmatize(word, pos='v')\n",
        "\n",
        "    def delete_auxilaries(words):\n",
        "        return [word for word in words if word not in auxilaries]\n",
        "\n",
        "    def process_sentence(sentence, pos_tags):\n",
        "        new_sent = []\n",
        "        for i, word in enumerate(sentence):\n",
        "            if pos_tags[i] in no_content:\n",
        "                continue\n",
        "            if pos_tags[i].startswith('V'):\n",
        "                new_sent.append(get_uninflected_form(word))\n",
        "            else:\n",
        "                new_sent.append(word)\n",
        "        return delete_auxilaries(new_sent)\n",
        "\n",
        "    new_p = process_sentence(p, dictionary['pos_p'])\n",
        "    new_h = process_sentence(h, dictionary['pos_h'])\n",
        "\n",
        "    return (new_p, new_h)"
      ],
      "metadata": {
        "id": "8qKBJr7jpUqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Wordnet functions"
      ],
      "metadata": {
        "id": "8M8KoKF4pFsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_antonyms(word):\n",
        "  \"\"\"Returns a list of antonyms for a given word.\"\"\"\n",
        "  antonyms = set()\n",
        "  for synset in wn.synsets(word):\n",
        "      for lemma in synset.lemmas():\n",
        "          if lemma.antonyms():\n",
        "              antonyms.update(ant.name() for ant in lemma.antonyms())\n",
        "  return list(antonyms)\n",
        "\n",
        "def get_all_hypernyms(synset):\n",
        "  \"\"\"Recursively get all hypernyms of a given synset.\"\"\"\n",
        "  hypernyms = set(synset.hypernyms())\n",
        "  for hypernym in synset.hypernyms():\n",
        "      hypernyms.update(get_all_hypernyms(hypernym))\n",
        "  return hypernyms\n",
        "\n",
        "def is_kind_of(word1, word2):\n",
        "  \"\"\"Checks if a given words is a hyponym of another given word\"\"\"\n",
        "  synsets_word1 = wn.synsets(word1)\n",
        "  synsets_word2 = wn.synsets(word2)\n",
        "\n",
        "  if not synsets_word1 or not synsets_word2:\n",
        "      return False\n",
        "\n",
        "  hypernyms_word1 = set()\n",
        "  for syn1 in synsets_word1:\n",
        "      hypernyms_word1.update(get_all_hypernyms(syn1))\n",
        "\n",
        "  for syn2 in synsets_word2:\n",
        "      if syn2 in hypernyms_word1:\n",
        "          return True\n",
        "\n",
        "  return False"
      ],
      "metadata": {
        "id": "jHLV1DUGjm0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Functions to extract parts from NLP trees"
      ],
      "metadata": {
        "id": "BapCekEqpcRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_first_NP(tree,in_pp):\n",
        "  \"\"\"Returns the first NP in a tree.\"\"\"\n",
        "  for sub in tree.subtrees(lambda t: t != tree):\n",
        "    if sub.label() == 'PP' or sub.label == 'ADVP':\n",
        "      in_pp.extend(sub.subtrees())\n",
        "    elif sub.label() == 'NP' and sub not in in_pp:\n",
        "      if sub.height() == 3:\n",
        "        for subsub in sub.subtrees(lambda t: t != tree):\n",
        "          if subsub.label() in nouns:\n",
        "            return sub\n",
        "      elif sub.height() > 3:\n",
        "        return find_first_NP(sub,in_pp)\n",
        "  return None\n",
        "\n",
        "def find_last_NP(tree,in_pp):\n",
        "  \"\"\"Returns the last NP in a tree.\"\"\"\n",
        "  NP = None\n",
        "  for sub in tree.subtrees(lambda t: t != tree):\n",
        "    if sub.label() == 'PP' or sub.label == 'ADVP':\n",
        "      in_pp.extend(sub.subtrees())\n",
        "    elif sub.label() == 'NP' and sub not in in_pp:\n",
        "      if sub.height() == 3:\n",
        "        NP = sub\n",
        "      elif sub.height() > 3:\n",
        "        NP = find_last_NP(sub,in_pp)\n",
        "  return NP\n",
        "\n",
        "def extract_last_noun_NP(NP):\n",
        "  \"\"\"Returns the last noun in a NP.\"\"\"\n",
        "  noun = None\n",
        "  if NP == None:\n",
        "    return None\n",
        "  for nn_subtree in NP.subtrees():\n",
        "    if nn_subtree.label() in nouns:\n",
        "      noun = nn_subtree[0].lower()\n",
        "  return noun\n",
        "\n",
        "def find_ADVPs(tree):\n",
        "  \"\"\"Returns all adverbial phrases in a tree.\"\"\"\n",
        "  ADVPs = []\n",
        "  for sub in tree.subtrees(lambda t: t != tree):\n",
        "    if sub.label() == 'ADVP' or sub.label() == 'ADJP':\n",
        "      if sub.height() == 3:\n",
        "        ADVPs.append(sub)\n",
        "      elif sub.height() > 3:\n",
        "        ADVPs.extend(find_ADVPs(sub))\n",
        "  return ADVPs"
      ],
      "metadata": {
        "id": "3OnoLdH1pKVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfpnUB5ufQ3O"
      },
      "source": [
        "#Our patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kw04MU0f3Yo"
      },
      "source": [
        "####Noun antonym patterns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def first_NP_antonym_pattern(p,h,dictionary):\n",
        "  \"\"\"The pattern that compares the last noun of the first NP of the premise and the hypothesis.\"\"\"\n",
        "  premise_tree = dictionary['tree_p']\n",
        "  hypothesis_tree = dictionary['tree_h']\n",
        "\n",
        "  first_NP_p = find_first_NP(premise_tree,[])\n",
        "  first_NP_h = find_first_NP(hypothesis_tree,[])\n",
        "\n",
        "  noun_p = extract_last_noun_NP(first_NP_p)\n",
        "  noun_h = extract_last_noun_NP(first_NP_h)\n",
        "\n",
        "  if noun_p and noun_h:\n",
        "    if noun_p in find_antonyms(noun_h) or noun_h in find_antonyms(noun_p):\n",
        "      return('contradiction',82)\n",
        "    if noun_p in extra_antonyms:\n",
        "      if noun_h in extra_antonyms[noun_p]:\n",
        "        return ('contradiction',88)\n",
        "    if noun_h in extra_antonyms:\n",
        "      if noun_p in extra_antonyms[noun_h]:\n",
        "        return ('contradiction',88)\n",
        "  return('none',0)"
      ],
      "metadata": {
        "id": "FciBm9Jddvnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def last_NP_antonym_pattern(p,h,dictionary):\n",
        "  \"\"\"The pattern that compares the last noun of the last NP of the premise and the hypothesis.\"\"\"\n",
        "  premise_tree = dictionary['tree_p']\n",
        "  hypothesis_tree = dictionary['tree_h']\n",
        "\n",
        "  last_NP_p = find_last_NP(premise_tree,[])\n",
        "  last_NP_h = find_last_NP(hypothesis_tree,[])\n",
        "\n",
        "  noun_p = extract_last_noun_NP(last_NP_p)\n",
        "  noun_h = extract_last_noun_NP(last_NP_h)\n",
        "\n",
        "  if noun_p and noun_h:\n",
        "    if noun_p in find_antonyms(noun_h) or noun_h in find_antonyms(noun_p):\n",
        "      return('contradiction',77)\n",
        "    if noun_p in extra_antonyms:\n",
        "      if noun_h in extra_antonyms[noun_p]:\n",
        "        return ('contradiction',80)\n",
        "    if noun_h in extra_antonyms:\n",
        "      if noun_p in extra_antonyms[noun_h]:\n",
        "        return ('contradiction',80)\n",
        "  return('none',0)"
      ],
      "metadata": {
        "id": "-E58OD3ZREst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tajdk3jaf8hv"
      },
      "source": [
        "####One word different patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVdSQXwUViYO"
      },
      "outputs": [],
      "source": [
        "def one_word_different(p,h):\n",
        "  \"\"\"Checks of there is a one word difference between the premise and hypothesis.\"\"\"\n",
        "  diff_words = (-1,-1)\n",
        "\n",
        "  len_diff = len(p) - len(h)\n",
        "  if abs(len_diff) > 1:\n",
        "    return((False,[]))\n",
        "\n",
        "  i, j, differences = 0, 0, 0\n",
        "\n",
        "  while i < len(p) and j < len(h):\n",
        "    if p[i] != h[j]:\n",
        "      differences += 1\n",
        "      if differences > 1:\n",
        "        return((False,[]))\n",
        "      if len_diff > 0:\n",
        "        diff_words = (i,-1)\n",
        "        i += 1\n",
        "      elif len_diff < 0:\n",
        "        diff_words = (-1,j)\n",
        "        j += 1\n",
        "      else:\n",
        "        diff_words = (i,j)\n",
        "        i += 1\n",
        "        j += 1\n",
        "    else:\n",
        "      i += 1\n",
        "      j += 1\n",
        "\n",
        "  differences += (len(p) - i) + (len(h) - j)\n",
        "\n",
        "  if (differences == 1):\n",
        "    return (True,diff_words)\n",
        "  return (False,[])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_different_pattern(p,h,dictionary):\n",
        "  \"\"\"The pattern that labels examples with a one word difference between the premise and hypothesis.\"\"\"\n",
        "  one_diff, diffs = one_word_different(p,h)\n",
        "  if not one_diff:\n",
        "    return ('none',0)\n",
        "  p_word_i, h_word_i = diffs\n",
        "  p_word = p[p_word_i]\n",
        "  h_word = h[h_word_i]\n",
        "\n",
        "  if p_word_i == -1:\n",
        "    if dictionary['pos_h'][h_word_i] in adj_adv:\n",
        "      return('neutral',95)\n",
        "  elif h_word_i == -1:\n",
        "    if dictionary['pos_p'][p_word_i] in adj_adv:\n",
        "      return('entailment',97)\n",
        "  else:\n",
        "    if p_word in find_antonyms(h_word) or h_word in find_antonyms(p_word):\n",
        "      return ('contradiction',100)\n",
        "    if h_word in extra_antonyms:\n",
        "      if p_word in extra_antonyms[h_word]:\n",
        "        return ('contradiction',100)\n",
        "    if p_word in extra_antonyms:\n",
        "      if h_word in extra_antonyms[p_word]:\n",
        "        return ('contradiction',100)\n",
        "\n",
        "    if is_kind_of(p_word,h_word):\n",
        "      return ('entailment',96)\n",
        "    if h_word in extra_hyponyms:\n",
        "      if p_word in extra_hyponyms[h_word]:\n",
        "        return ('entailment',96)\n",
        "    if is_kind_of(h_word,p_word):\n",
        "      return ('neutral',57)\n",
        "    if p_word in extra_hyponyms:\n",
        "      if h_word in extra_hyponyms[p_word]:\n",
        "        return ('neutral',57)\n",
        "  return ('none',0)"
      ],
      "metadata": {
        "id": "SQe_MF2So3io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_different_contentword_pattern(p,h,dictionary):\n",
        "  \"\"\"The pattern that labels examples with a one word difference between the premise and hypothesis that only contain words with context.\"\"\"\n",
        "\n",
        "  (new_p, new_h) = dictionary['content_words']\n",
        "\n",
        "  output = one_different_pattern(new_p, new_h, dictionary)\n",
        "\n",
        "  if output == ('neutral', 95):\n",
        "    return ('neutral', 94)\n",
        "  if output == ('entailment', 97):\n",
        "    return ('entailment', 94)\n",
        "  if output == ('contradiction', 100):\n",
        "    return ('contradiction', 100)\n",
        "  if output == ('entailment', 96):\n",
        "    return ('entailment', 93)\n",
        "  if output == ('neutral', 57):\n",
        "    return ('neutral', 68)\n",
        "\n",
        "  return ('none', 0)"
      ],
      "metadata": {
        "id": "1sp3DnUD5CUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g42mdFSdgLpP"
      },
      "source": [
        "\n",
        "####Frequent words pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5-izZZtKJE7"
      },
      "outputs": [],
      "source": [
        "def frequent_words_pattern(p,h,dictionary):\n",
        "  \"\"\"The pattern that labels examples with frequently used words in sentences with that label.\"\"\"\n",
        "  for w in ['picture', 'least', 'instrument', 'sport', 'motion', 'interacting', 'vehicle', 'object', 'proximity']:\n",
        "    if w in h:\n",
        "      return ('entailment', 67)\n",
        "\n",
        "  for w in ['tall', 'joyously', 'championship', 'impress', 'winning', 'vacation', 'favorite', 'sad', 'siblings', 'win']:\n",
        "    if w in h:\n",
        "      return ('neutral', 88)\n",
        "\n",
        "  for w in ['tv', 'nobody', 'nothing', 'cats', 'sleeping', 'naked', 'quietly', 'no', 'mars', 'cat']:\n",
        "    if w in h:\n",
        "      return ('contradiction', 88)\n",
        "\n",
        "  return ('none',0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Substitute \"outdoors\" or \"outside\" pattern"
      ],
      "metadata": {
        "id": "rCAeYvLDqxbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def substitution_pattern(p,h,dictionary):\n",
        "  \"\"\"The pattern that labels examples with substitution of \"outdoors\" or \"outside\".\"\"\"\n",
        "  frequent_pps = [\"outside\",\"outdoors\"]\n",
        "\n",
        "  for word in frequent_pps:\n",
        "    if word in h:\n",
        "      tree_h = dictionary['tree_h']\n",
        "      for advp in find_ADVPs(tree_h):\n",
        "        if advp.leaves() == [word]:\n",
        "          if word == \"outdoors\":\n",
        "            return('entailment',91)\n",
        "          if word == \"outside\":\n",
        "            return('entailment',69)\n",
        "\n",
        "  return('none',0)"
      ],
      "metadata": {
        "id": "zb7PE6uwy3M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Same words pattern"
      ],
      "metadata": {
        "id": "Vqhjl6_5k3Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def same_words_pattern(p, h, dictionary):\n",
        "  \"\"\"The pattern that labels examples where the words in the hypothesis are also in the premise.\"\"\"\n",
        "  (rest_p, rest_h) = dictionary['content_words']\n",
        "\n",
        "  for word in rest_h:\n",
        "    if word not in rest_p:\n",
        "      return ('none', 0)\n",
        "  return ('entailment', 95)"
      ],
      "metadata": {
        "id": "bnTxDiDsk250"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Count negations pattern"
      ],
      "metadata": {
        "id": "yweXosxwwBLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_negations(string):\n",
        "  \"\"\"Counts the number of negations in a string.\"\"\"\n",
        "  negation_pattern = r\"\\bnot\\b|\\bnone\\b|\\bnobody\\b|\\bcannot\\b|\\b\\w+n['’]t\\b|\\bnever\\b|\\bno\\b|\\bnowhere\\b|\\bneither\\b\"\n",
        "  matches = re.findall(negation_pattern, string, flags=re.IGNORECASE)\n",
        "  return len(matches)\n",
        "\n",
        "def negation_pattern(p, h, dictionary):\n",
        "  \"\"\"The pattern that labels examples according to the number of negations in the sentences.\"\"\"\n",
        "  if count_negations(\" \".join(p)) != count_negations(\" \".join(h)):\n",
        "    return ('contradiction', 70)\n",
        "  else:\n",
        "    return ('none', 0)"
      ],
      "metadata": {
        "id": "lpjzRTAaqhYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spTTiS4AgC3g"
      },
      "source": [
        "#Our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll4kmeDFEF_D"
      },
      "outputs": [],
      "source": [
        "def run_patterns(patterns,data):\n",
        "  \"\"\"Runs all the patterns on the data.\"\"\"\n",
        "  number_correct, number_covered = 0, 0\n",
        "  correct_c, correct_n, correct_e = 0, 0, 0\n",
        "  covered_c, covered_n, covered_e = 0, 0 ,0\n",
        "  for key in data:\n",
        "    dictionary = {}\n",
        "    item = data[key]\n",
        "    s2a_p = S2A[item['p']]\n",
        "    s2a_h = S2A[item['h']]\n",
        "    p = [word.lower() for word in s2a_p['tok']]\n",
        "    h = [word.lower() for word in s2a_h['tok']]\n",
        "    dictionary['pos_p'] = s2a_p['pos']\n",
        "    dictionary['pos_h'] = s2a_h['pos']\n",
        "    dictionary['tree_p'] = Tree.fromstring(s2a_p['tree'])\n",
        "    dictionary['tree_h'] = Tree.fromstring(s2a_h['tree'])\n",
        "    dictionary['content_words'] = process_text(p,h,dictionary)\n",
        "\n",
        "    labels = []\n",
        "    for f in patterns:\n",
        "      prediction, precision = f(p,h,dictionary)\n",
        "      labels.append((prediction,precision))\n",
        "\n",
        "    highest_precision = 0\n",
        "    label = 'none'\n",
        "    for (prediction, precision) in labels:\n",
        "      if precision > highest_precision:\n",
        "        label = prediction\n",
        "        highest_precision = precision\n",
        "\n",
        "    if label != 'none':\n",
        "      number_covered += 1\n",
        "      if label == 'contradiction':\n",
        "        covered_c += 1\n",
        "      if label == 'entailment':\n",
        "        covered_e += 1\n",
        "      if label == 'neutral':\n",
        "        covered_n += 1\n",
        "      if label == item['g']:\n",
        "        number_correct += 1\n",
        "        if label == 'contradiction':\n",
        "          correct_c += 1\n",
        "        if label == 'entailment':\n",
        "          correct_e += 1\n",
        "        if label == 'neutral':\n",
        "          correct_n += 1\n",
        "\n",
        "  print(f\"{number_covered} covered from {len(data)} \\n\"\n",
        "  f\"{number_correct} correct labels \\n\"\n",
        "  f\"{number_covered-number_correct} wrong labels \\n\"\n",
        "  f\"precision: {number_correct/number_covered} \\n\\n\"\n",
        "  f\"{correct_c} correct contradiction labels from {covered_c} covered \\n\"\n",
        "  f\"{correct_e} correct entailment labels from {covered_e} covered \\n\"\n",
        "  f\"{correct_n} correct neutral labels from {covered_n} covered \\n\")\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patterns = [first_NP_antonym_pattern, last_NP_antonym_pattern,\n",
        "            one_different_pattern, one_different_contentword_pattern,\n",
        "            same_words_pattern, negation_pattern, frequent_words_pattern, substitution_pattern]\n",
        "\n",
        "run_patterns(patterns,SNLI['test'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ybTES9dxNpv",
        "outputId": "3b70e370-1bb6-4d24-aa10-36d526481199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2398 covered from 9824 \n",
            "2006 correct labels \n",
            "392 wrong labels \n",
            "precision: 0.8365304420350292 \n",
            "\n",
            "646 correct contradiction labels from 821 covered \n",
            "1243 correct entailment labels from 1446 covered \n",
            "117 correct neutral labels from 131 covered \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}